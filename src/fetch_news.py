#!/usr/bin/env python3
"""
Global News Fetcher with Translation
ä» RSS æºæŠ“å–æ–°é—»ï¼Œç¿»è¯‘å¹¶ç”Ÿæˆ JSON æ•°æ®
"""

import feedparser
import json
from datetime import datetime, timezone
from pathlib import Path
import hashlib
import time
import re
import urllib.request
import urllib.parse

# ç¿»è¯‘ API (ä½¿ç”¨ MyMemory å…è´¹ API)
TRANSLATE_API = "https://api.mymemory.translated.net/get"

def translate_text(text: str, source_lang: str = "en", target_lang: str = "zh") -> str:
    """ç¿»è¯‘æ–‡æœ¬ (ä½¿ç”¨ MyMemory å…è´¹ API)"""
    if not text or len(text.strip()) == 0:
        return text
    
    # å¦‚æœå·²ç»æ˜¯ä¸­æ–‡ï¼Œä¸éœ€è¦ç¿»è¯‘
    if contains_chinese(text) and target_lang == "zh":
        return text
    
    try:
        # é™åˆ¶æ–‡æœ¬é•¿åº¦ (API é™åˆ¶ 500 å­—ç¬¦)
        text = text[:500]
        
        # æ„å»ºè¯·æ±‚
        params = urllib.parse.urlencode({
            'q': text,
            'langpair': f"{source_lang}|{target_lang}"
        })
        
        url = f"{TRANSLATE_API}?{params}"
        
        # å‘é€è¯·æ±‚
        req = urllib.request.Request(url, headers={'User-Agent': 'GlobalNewsFetcher/1.0'})
        with urllib.request.urlopen(req, timeout=5) as response:
            data = json.loads(response.read().decode('utf-8'))
            
            if 'responseData' in data and 'translatedText' in data['responseData']:
                translated = data['responseData']['translatedText']
                # æ¸…ç†ç¿»è¯‘ç»“æœ
                translated = re.sub(r'\s+', ' ', translated).strip()
                return translated
    except Exception as e:
        print(f"âš ï¸  Translation error: {e}")
    
    # ç¿»è¯‘å¤±è´¥æ—¶è¿”å›åŸæ–‡
    return text

def contains_chinese(text: str) -> bool:
    """æ£€æŸ¥æ–‡æœ¬æ˜¯å¦åŒ…å«ä¸­æ–‡"""
    return bool(re.search(r'[\u4e00-\u9fff]', text))

def parse_rss_feed(url: str, source_name: str, source_lang: str = "en") -> list:
    """è§£æ RSS æº"""
    try:
        feed = feedparser.parse(url)
        articles = []
        
        for entry in feed.entries[:15]:  # æ¯ä¸ªæºå– 15 æ¡
            # è§£ææ—¶é—´
            published = ''
            if hasattr(entry, 'published_parsed') and entry.published_parsed:
                try:
                    dt = datetime(*entry.published_parsed[:6], tzinfo=timezone.utc)
                    published = dt.isoformat()
                except:
                    published = entry.get('published', '')
            
            # æ¸…ç† HTML æ ‡ç­¾
            title = re.sub(r'<[^>]+>', '', entry.title).strip()
            summary = re.sub(r'<[^>]+>', '', entry.get('summary', entry.get('description', ''))).strip()[:300]
            
            article = {
                'id': hashlib.md5(entry.link.encode()).hexdigest()[:12],
                'title': title,
                'link': entry.link,
                'published': published,
                'source': source_name,
                'summary': summary,
                'original_lang': source_lang
            }
            articles.append(article)
        
        return articles
    except Exception as e:
        print(f"âŒ Error fetching {source_name}: {e}")
        return []

def translate_articles(articles: list, target_lang: str = "zh") -> list:
    """æ‰¹é‡ç¿»è¯‘æ–°é—»"""
    print(f"ğŸŒ Translating {len(articles)} articles to {target_lang}...")
    
    translated_count = 0
    for i, article in enumerate(articles, 1):
        # è·³è¿‡å·²ç»æ˜¯ä¸­æ–‡çš„æ–‡ç« 
        if article.get('original_lang') == 'zh' or contains_chinese(article['title']):
            article['title_zh'] = article['title']
            article['summary_zh'] = article['summary']
            continue
        
        # ç¿»è¯‘æ ‡é¢˜å’Œæ‘˜è¦
        print(f"  [{i}/{len(articles)}] Translating: {article['title'][:50]}...")
        
        article['title_zh'] = translate_text(article['title'], 'en', target_lang)
        time.sleep(0.3)  # é¿å… API é™æµ
        
        if article['summary']:
            article['summary_zh'] = translate_text(article['summary'], 'en', target_lang)
            time.sleep(0.3)
        else:
            article['summary_zh'] = ''
        
        translated_count += 1
    
    print(f"âœ… Translated {translated_count} articles")
    return articles

def deduplicate_articles(articles: list) -> list:
    """å»é‡ (åŸºäº ID)"""
    seen = set()
    unique = []
    
    for article in articles:
        if article['id'] not in seen:
            seen.add(article['id'])
            unique.append(article)
    
    return unique

def main():
    print("ğŸŒ Global News Fetcher started")
    print("=" * 50)
    
    # åŠ è½½é…ç½®
    sources_file = Path('src/sources.json')
    if not sources_file.exists():
        print(f"âŒ Config file not found: {sources_file}")
        return
    
    with open(sources_file, 'r', encoding='utf-8') as f:
        config = json.load(f)
    
    all_articles = []
    
    # æŠ“å–æ‰€æœ‰æº
    for i, source in enumerate(config['sources'], 1):
        print(f"[{i}/{len(config['sources'])}] Fetching {source['name']}...")
        articles = parse_rss_feed(source['rss'], source['name'], source.get('language', 'en'))
        
        # æ·»åŠ åˆ†ç±»ä¿¡æ¯
        for article in articles:
            article['categories'] = source.get('categories', ['general'])
            article['category'] = source.get('categories', ['general'])[0]  # ä¸»åˆ†ç±»
            article['country'] = source.get('country', 'US')
            article['language'] = source.get('language', 'en')
        
        all_articles.extend(articles)
        time.sleep(0.5)  # é¿å…è¯·æ±‚è¿‡å¿«
    
    print("=" * 50)
    
    # å»é‡
    unique_articles = deduplicate_articles(all_articles)
    print(f"ğŸ“° Total articles: {len(all_articles)}")
    print(f"âœ¨ Unique articles: {len(unique_articles)}")
    
    # ç¿»è¯‘æ–‡ç«  (è‹±æ–‡â†’ä¸­æ–‡)
    unique_articles = translate_articles(unique_articles, 'zh')
    
    # æŒ‰æ—¶é—´æ’åº
    unique_articles.sort(
        key=lambda x: x.get('published', ''),
        reverse=True
    )
    
    # ç”Ÿæˆè¾“å‡º
    output = {
        'updated': datetime.now(timezone.utc).isoformat(),
        'total': len(unique_articles),
        'sources_count': len(config['sources']),
        'category_groups': config.get('categoryGroups', {}),
        'articles': unique_articles[:100]  # åªä¿ç•™æœ€æ–° 100 æ¡
    }
    
    # ä¿å­˜åˆ° data/news.json
    output_file = Path('data/news.json')
    output_file.parent.mkdir(exist_ok=True)
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(output, f, indent=2, ensure_ascii=False)
    
    print(f"âœ… Saved to {output_file}")
    print(f"ğŸ• Updated at: {output['updated']}")

if __name__ == '__main__':
    main()
