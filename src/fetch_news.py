#!/usr/bin/env python3
"""
Global News Fetcher with Alibaba Cloud Bailian (Qwen) Translation
ä» RSS æºæŠ“å–æ–°é—»ï¼Œä½¿ç”¨é˜¿é‡Œäº‘ç™¾ç‚¼ Qwen æ¨¡å‹ç¿»è¯‘å¹¶ç”Ÿæˆ JSON æ•°æ®

ä¼˜åŒ–ç‚¹:
- æ‰¹é‡ç¿»è¯‘ (å‡å°‘ API è°ƒç”¨æ¬¡æ•°)
- æ™ºèƒ½å»é‡ (æ ‡é¢˜ç›¸ä¼¼åº¦æ£€æµ‹)
- é”™è¯¯é‡è¯•æœºåˆ¶
- ç¿»è¯‘ç¼“å­˜ (é¿å…é‡å¤ç¿»è¯‘ç›¸åŒå†…å®¹)
"""

import feedparser
import json
from datetime import datetime, timezone
from pathlib import Path
import hashlib
import time
import re
import urllib.request
import urllib.error
import os
from typing import List, Dict, Optional

# é˜¿é‡Œäº‘ç™¾ç‚¼ API (Qwen)
DASHSCOPE_API_URL = "https://dashscope.aliyuncs.com/api/v1/services/aigc/text-generation/generation"

# ç¿»è¯‘é…ç½®
USE_TRANSLATION = True
TRANSLATION_MODEL = "qwen-turbo"  # å¿«é€Ÿä¸”ä¾¿å®œ
BATCH_TRANSLATE = True  # å¯ç”¨æ‰¹é‡ç¿»è¯‘
MAX_BATCH_SIZE = 10  # æ¯æ‰¹ç¿»è¯‘çš„æ–‡ç« æ•°
CACHE_TRANSLATION = True  # å¯ç”¨ç¿»è¯‘ç¼“å­˜

def get_api_key() -> str:
    """è·å–é˜¿é‡Œäº‘ API Key"""
    # 1. ä»ç¯å¢ƒå˜é‡
    if os.environ.get('DASHSCOPE_API_KEY'):
        return os.environ['DASHSCOPE_API_KEY']
    
    # 2. ä» OpenClaw auth-profiles.json
    auth_file = Path.home() / '.openclaw' / 'agents' / 'main' / 'agent' / 'auth-profiles.json'
    if auth_file.exists():
        try:
            with open(auth_file, 'r') as f:
                auth = json.load(f)
                if 'dashscope' in auth and auth['dashscope'].get('apiKey'):
                    return auth['dashscope']['apiKey']
        except:
            pass
    
    # 3. ä»é¡¹ç›®é…ç½®
    config_file = Path('config.json')
    if config_file.exists():
        try:
            with open(config_file, 'r') as f:
                config = json.load(f)
                if config.get('dashscope_api_key'):
                    return config['dashscope_api_key']
        except:
            pass
    
    return ""

def load_translation_cache() -> Dict[str, str]:
    """åŠ è½½ç¿»è¯‘ç¼“å­˜"""
    cache_file = Path('data/translation_cache.json')
    if cache_file.exists():
        try:
            with open(cache_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        except:
            pass
    return {}

def save_translation_cache(cache: Dict[str, str]):
    """ä¿å­˜ç¿»è¯‘ç¼“å­˜"""
    cache_file = Path('data/translation_cache.json')
    cache_file.parent.mkdir(exist_ok=True)
    try:
        with open(cache_file, 'w', encoding='utf-8') as f:
            json.dump(cache, f, indent=2, ensure_ascii=False)
    except:
        pass

def translate_text_batch(texts: List[str], api_key: str, source_lang: str = "en", target_lang: str = "zh") -> List[str]:
    """æ‰¹é‡ç¿»è¯‘æ–‡æœ¬"""
    if not texts:
        return []
    
    # è¿‡æ»¤æ‰å·²ç»æ˜¯ä¸­æ–‡æˆ–ç©ºçš„å†…å®¹
    to_translate = []
    indices = []
    for i, text in enumerate(texts):
        if not text or len(text.strip()) == 0:
            continue
        if contains_chinese(text) or not is_mostly_english(text):
            continue
        to_translate.append(text[:500])  # é™åˆ¶é•¿åº¦
        indices.append(i)
    
    if not to_translate:
        return texts
    
    # æ‰¹é‡ç¿»è¯‘
    batch_prompt = "Translate the following texts from English to Chinese. Output ONLY the translations, one per line, in the same order:\n\n"
    for i, text in enumerate(to_translate):
        batch_prompt += f"{i+1}. {text}\n"
    
    try:
        data = json.dumps({
            "model": TRANSLATION_MODEL,
            "input": {
                "messages": [
                    {"role": "system", "content": "You are a professional translator. Translate accurately and concisely."},
                    {"role": "user", "content": batch_prompt}
                ]
            },
            "parameters": {
                "temperature": 0.3,
                "max_tokens": 2048
            }
        }).encode('utf-8')
        
        req = urllib.request.Request(
            DASHSCOPE_API_URL,
            data=data,
            headers={
                'Content-Type': 'application/json',
                'Authorization': f'Bearer {api_key}'
            },
            method='POST'
        )
        
        with urllib.request.urlopen(req, timeout=60) as response:
            result = json.loads(response.read().decode('utf-8'))
            
            if 'output' in result and 'choices' in result['output']:
                translated_text = result['output']['choices'][0]['message']['content'].strip()
                # è§£æç¿»è¯‘ç»“æœ
                translations = {}
                for line in translated_text.split('\n'):
                    match = re.match(r'^\d+\.\s*(.+)$', line.strip())
                    if match:
                        translations[len(translations)] = match.group(1).strip()
                
                # æ„å»ºç»“æœ
                results = list(texts)
                for i, idx in enumerate(indices):
                    if i in translations:
                        results[idx] = translations[i]
                
                return results
    
    except Exception as e:
        print(f"âš ï¸  Batch translation error: {e}")
    
    return texts

def translate_text(text: str, api_key: str, cache: Dict[str, str], source_lang: str = "en", target_lang: str = "zh") -> str:
    """ç¿»è¯‘å•ä¸ªæ–‡æœ¬ (å¸¦ç¼“å­˜)"""
    if not text or len(text.strip()) == 0:
        return text
    
    # å¦‚æœå·²ç»æ˜¯ä¸­æ–‡ï¼Œè·³è¿‡
    if contains_chinese(text) and target_lang == "zh":
        return text
    
    # å¦‚æœä¸æ˜¯è‹±æ–‡ï¼Œè·³è¿‡
    if not is_mostly_english(text):
        return text
    
    # æ£€æŸ¥ç¼“å­˜
    cache_key = hashlib.md5(text.encode()).hexdigest()
    if CACHE_TRANSLATION and cache_key in cache:
        return cache[cache_key]
    
    # å•ä¸ªç¿»è¯‘ (fallback)
    try:
        data = json.dumps({
            "model": TRANSLATION_MODEL,
            "input": {
                "messages": [
                    {"role": "system", "content": "You are a professional translator. Translate from English to Chinese. Output ONLY the translation."},
                    {"role": "user", "content": text[:500]}
                ]
            },
            "parameters": {
                "temperature": 0.3,
                "max_tokens": 512
            }
        }).encode('utf-8')
        
        req = urllib.request.Request(
            DASHSCOPE_API_URL,
            data=data,
            headers={
                'Content-Type': 'application/json',
                'Authorization': f'Bearer {api_key}'
            },
            method='POST'
        )
        
        with urllib.request.urlopen(req, timeout=30) as response:
            result = json.loads(response.read().decode('utf-8'))
            
            if 'output' in result and 'choices' in result['output']:
                translated = result['output']['choices'][0]['message']['content'].strip()
                translated = re.sub(r'\s+', ' ', translated).strip()
                
                # ä¿å­˜åˆ°ç¼“å­˜
                if CACHE_TRANSLATION and translated != text:
                    cache[cache_key] = translated
                
                return translated
    
    except Exception as e:
        print(f"âš ï¸  Translation error: {e}")
    
    return text

def contains_chinese(text: str) -> bool:
    """æ£€æŸ¥æ–‡æœ¬æ˜¯å¦åŒ…å«ä¸­æ–‡"""
    return bool(re.search(r'[\u4e00-\u9fff]', text))

def is_mostly_english(text: str) -> bool:
    """æ£€æŸ¥æ–‡æœ¬æ˜¯å¦ä¸»è¦æ˜¯è‹±æ–‡"""
    if not text:
        return False
    english_chars = sum(1 for c in text if c.isascii() and c.isalpha())
    ratio = english_chars / len(text) if len(text) > 0 else 0
    return ratio > 0.8

def parse_rss_feed(url: str, source_name: str, source_lang: str = "en") -> List[Dict]:
    """è§£æ RSS æº (å¸¦é‡è¯•)"""
    max_retries = 2
    for attempt in range(max_retries):
        try:
            feed = feedparser.parse(url)
            articles = []
            
            for entry in feed.entries[:15]:
                published = ''
                if hasattr(entry, 'published_parsed') and entry.published_parsed:
                    try:
                        dt = datetime(*entry.published_parsed[:6], tzinfo=timezone.utc)
                        published = dt.isoformat()
                    except:
                        published = entry.get('published', '')
                
                title = re.sub(r'<[^>]+>', '', entry.title).strip()
                summary = re.sub(r'<[^>]+>', '', entry.get('summary', entry.get('description', ''))).strip()[:300]
                
                article = {
                    'id': hashlib.md5(entry.link.encode()).hexdigest()[:12],
                    'title': title,
                    'link': entry.link,
                    'published': published,
                    'source': source_name,
                    'summary': summary,
                    'original_lang': source_lang
                }
                articles.append(article)
            
            return articles
        
        except Exception as e:
            if attempt < max_retries - 1:
                print(f"  âš ï¸  Retry {attempt + 1}/{max_retries} for {source_name}")
                time.sleep(1)
            else:
                print(f"âŒ Error fetching {source_name}: {e}")
    
    return []

def deduplicate_articles(articles: List[Dict]) -> List[Dict]:
    """å»é‡ (åŸºäº ID + æ ‡é¢˜ç›¸ä¼¼åº¦)"""
    seen_ids = set()
    seen_titles = set()
    unique = []
    
    for article in articles:
        # ID å»é‡
        if article['id'] in seen_ids:
            continue
        
        # æ ‡é¢˜ç›¸ä¼¼åº¦å»é‡ (ç®€åŒ–ç‰ˆï¼šå°å†™ + å»æ ‡ç‚¹)
        title_key = re.sub(r'[^\w\s]', '', article['title'].lower())[:50]
        if title_key in seen_titles:
            continue
        
        seen_ids.add(article['id'])
        seen_titles.add(title_key)
        unique.append(article)
    
    return unique

def translate_articles(articles: List[Dict], api_key: str, cache: Dict[str, str], target_lang: str = "zh") -> List[Dict]:
    """æ‰¹é‡ç¿»è¯‘æ–‡ç« """
    if not USE_TRANSLATION or not api_key:
        print("â­ï¸  è·³è¿‡ç¿»è¯‘")
        return articles
    
    print(f"ğŸŒ ä½¿ç”¨é˜¿é‡Œäº‘ Qwen ç¿»è¯‘ {len(articles)} ç¯‡æ–‡ç« ...")
    
    translated_count = 0
    failed_count = 0
    skipped_count = 0
    
    if BATCH_TRANSLATE:
        # æ‰¹é‡ç¿»è¯‘æ¨¡å¼
        for i in range(0, len(articles), MAX_BATCH_SIZE):
            batch = articles[i:i + MAX_BATCH_SIZE]
            titles = [a['title'] for a in batch]
            summaries = [a['summary'] for a in batch if a['summary']]
            
            # ç¿»è¯‘æ ‡é¢˜
            translated_titles = translate_text_batch(titles, api_key)
            for j, article in enumerate(batch):
                if article.get('original_lang') == 'zh' or contains_chinese(article['title']):
                    article['title_zh'] = article['title']
                    skipped_count += 1
                else:
                    article['title_zh'] = translated_titles[j] if j < len(translated_titles) else article['title']
                    if article['title_zh'] != article['title']:
                        translated_count += 1
                    else:
                        failed_count += 1
            
            # ç¿»è¯‘æ‘˜è¦
            if summaries:
                translated_summaries = translate_text_batch(summaries, api_key)
                summary_idx = 0
                for j, article in enumerate(batch):
                    if article['summary']:
                        if article.get('original_lang') == 'zh' or contains_chinese(article['summary']):
                            article['summary_zh'] = article['summary']
                        else:
                            article['summary_zh'] = translated_summaries[summary_idx] if summary_idx < len(translated_summaries) else article['summary']
                        summary_idx += 1
            
            print(f"  æ‰¹æ¬¡ {i//MAX_BATCH_SIZE + 1}/{(len(articles) + MAX_BATCH_SIZE - 1)//MAX_BATCH_SIZE} å®Œæˆ")
            time.sleep(0.5)  # API é™æµä¿æŠ¤
    
    else:
        # å•ä¸ªç¿»è¯‘æ¨¡å¼ (æ—§ç‰ˆ)
        for i, article in enumerate(articles, 1):
            if article.get('original_lang') == 'zh' or contains_chinese(article['title']):
                article['title_zh'] = article['title']
                article['summary_zh'] = article['summary']
                skipped_count += 1
                continue
            
            print(f"  [{i}/{len(articles)}] {article['source']}: {article['title'][:40]}...")
            
            article['title_zh'] = translate_text(article['title'], api_key, cache)
            time.sleep(0.2)
            
            if article['summary']:
                article['summary_zh'] = translate_text(article['summary'], api_key, cache)
                time.sleep(0.2)
            else:
                article['summary_zh'] = ''
            
            if article['title_zh'] != article['title']:
                translated_count += 1
            else:
                failed_count += 1
    
    print(f"âœ… ç¿»è¯‘å®Œæˆï¼š{translated_count} ç¯‡æˆåŠŸï¼Œ{failed_count} ç¯‡å¤±è´¥ï¼Œ{skipped_count} ç¯‡è·³è¿‡ (å·²æ˜¯ä¸­æ–‡)")
    return articles

def main():
    print("ğŸŒ Global News Fetcher (é˜¿é‡Œäº‘ç™¾ç‚¼ Qwen ç¿»è¯‘)")
    print("=" * 50)
    
    # æ£€æŸ¥ API Key
    api_key = get_api_key()
    if api_key:
        print(f"âœ… é˜¿é‡Œäº‘ API Key å·²é…ç½®")
    else:
        print("âš ï¸  æœªé…ç½®é˜¿é‡Œäº‘ API Key")
        print("   è·å–æ–¹å¼ï¼šhttps://bailian.console.aliyun.com/")
    
    # åŠ è½½é…ç½®
    sources_file = Path('src/sources.json')
    if not sources_file.exists():
        print(f"âŒ Config file not found: {sources_file}")
        return
    
    with open(sources_file, 'r', encoding='utf-8') as f:
        config = json.load(f)
    
    all_articles = []
    
    # æŠ“å–æ‰€æœ‰æº
    for i, source in enumerate(config['sources'], 1):
        print(f"[{i}/{len(config['sources'])}] Fetching {source['name']}...")
        articles = parse_rss_feed(source['rss'], source['name'], source.get('language', 'en'))
        
        # æ·»åŠ åˆ†ç±»ä¿¡æ¯
        for article in articles:
            article['categories'] = source.get('categories', ['general'])
            article['category'] = source.get('categories', ['general'])[0]
            article['country'] = source.get('country', 'US')
            article['language'] = source.get('language', 'en')
        
        all_articles.extend(articles)
        time.sleep(0.2)  # é¿å…è¯·æ±‚è¿‡å¿«
    
    print("=" * 50)
    
    # å»é‡
    unique_articles = deduplicate_articles(all_articles)
    print(f"ğŸ“° Total articles: {len(all_articles)}")
    print(f"âœ¨ Unique articles: {len(unique_articles)}")
    
    # åŠ è½½ç¿»è¯‘ç¼“å­˜
    cache = load_translation_cache() if CACHE_TRANSLATION else {}
    
    # ç¿»è¯‘æ–‡ç« 
    unique_articles = translate_articles(unique_articles, api_key, cache, 'zh')
    
    # ä¿å­˜ç¿»è¯‘ç¼“å­˜
    if CACHE_TRANSLATION:
        save_translation_cache(cache)
    
    # æŒ‰æ—¶é—´æ’åº
    unique_articles.sort(
        key=lambda x: x.get('published', ''),
        reverse=True
    )
    
    # ç”Ÿæˆè¾“å‡º
    output = {
        'updated': datetime.now(timezone.utc).isoformat(),
        'total': len(unique_articles),
        'sources_count': len(config['sources']),
        'category_groups': config.get('categoryGroups', {}),
        'articles': unique_articles[:150]  # æœ€å¤šä¿ç•™ 150 ç¯‡
    }
    
    # ä¿å­˜åˆ° data/news.json
    output_file = Path('data/news.json')
    output_file.parent.mkdir(exist_ok=True)
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(output, f, indent=2, ensure_ascii=False)
    
    print(f"âœ… Saved to {output_file}")
    print(f"ğŸ• Updated at: {output['updated']}")

if __name__ == '__main__':
    main()
