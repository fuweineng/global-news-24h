#!/usr/bin/env python3
"""
Global News Fetcher with Local Translation (Ollama)
ä» RSS æºæŠ“å–æ–°é—»ï¼Œä½¿ç”¨æœ¬åœ° Ollama ç¿»è¯‘å¹¶ç”Ÿæˆ JSON æ•°æ®
"""

import feedparser
import json
from datetime import datetime, timezone
from pathlib import Path
import hashlib
import time
import re
import urllib.request
import urllib.error

# æœ¬åœ° Ollama API
OLLAMA_API = "http://localhost:11434/api/generate"

def translate_text(text: str, source_lang: str = "en", target_lang: str = "zh") -> str:
    """ä½¿ç”¨æœ¬åœ° Ollama ç¿»è¯‘æ–‡æœ¬"""
    if not text or len(text.strip()) == 0:
        return text
    
    # å¦‚æœå·²ç»æ˜¯ä¸­æ–‡ï¼Œä¸éœ€è¦ç¿»è¯‘
    if contains_chinese(text) and target_lang == "zh":
        return text
    
    # åªç¿»è¯‘çº¯è‹±æ–‡å†…å®¹
    if not is_mostly_english(text):
        return text
    
    try:
        # é™åˆ¶æ–‡æœ¬é•¿åº¦
        text = text[:400]
        
        # æ„å»ºæç¤ºè¯
        prompt = f"Translate the following text from {source_lang} to {target_lang}. Only output the translation, nothing else:\n\n{text}"
        
        # æ„å»ºè¯·æ±‚
        data = json.dumps({
            "model": "qwen2.5:7b",
            "prompt": prompt,
            "stream": False,
            "options": {
                "temperature": 0.3,
                "num_predict": 512
            }
        }).encode('utf-8')
        
        req = urllib.request.Request(
            OLLAMA_API,
            data=data,
            headers={'Content-Type': 'application/json'},
            method='POST'
        )
        
        with urllib.request.urlopen(req, timeout=30) as response:
            result = json.loads(response.read().decode('utf-8'))
            
            if 'response' in result:
                translated = result['response'].strip()
                # æ¸…ç†ç¿»è¯‘ç»“æœ
                translated = re.sub(r'\s+', ' ', translated).strip()
                if translated and translated != text:
                    return translated
    
    except Exception as e:
        print(f"âš ï¸  Translation error: {e}")
    
    # ç¿»è¯‘å¤±è´¥æ—¶è¿”å›åŸæ–‡
    return text

def contains_chinese(text: str) -> bool:
    """æ£€æŸ¥æ–‡æœ¬æ˜¯å¦åŒ…å«ä¸­æ–‡"""
    return bool(re.search(r'[\u4e00-\u9fff]', text))

def is_mostly_english(text: str) -> bool:
    """æ£€æŸ¥æ–‡æœ¬æ˜¯å¦ä¸»è¦æ˜¯è‹±æ–‡"""
    if not text:
        return False
    english_chars = sum(1 for c in text if c.isascii() and c.isalpha())
    ratio = english_chars / len(text) if len(text) > 0 else 0
    return ratio > 0.8

def parse_rss_feed(url: str, source_name: str, source_lang: str = "en") -> list:
    """è§£æ RSS æº"""
    try:
        feed = feedparser.parse(url)
        articles = []
        
        for entry in feed.entries[:15]:
            published = ''
            if hasattr(entry, 'published_parsed') and entry.published_parsed:
                try:
                    dt = datetime(*entry.published_parsed[:6], tzinfo=timezone.utc)
                    published = dt.isoformat()
                except:
                    published = entry.get('published', '')
            
            title = re.sub(r'<[^>]+>', '', entry.title).strip()
            summary = re.sub(r'<[^>]+>', '', entry.get('summary', entry.get('description', ''))).strip()[:300]
            
            article = {
                'id': hashlib.md5(entry.link.encode()).hexdigest()[:12],
                'title': title,
                'link': entry.link,
                'published': published,
                'source': source_name,
                'summary': summary,
                'original_lang': source_lang
            }
            articles.append(article)
        
        return articles
    except Exception as e:
        print(f"âŒ Error fetching {source_name}: {e}")
        return []

def translate_articles(articles: list, target_lang: str = "zh") -> list:
    """æ‰¹é‡ç¿»è¯‘æ–°é—»"""
    print(f"ğŸŒ Translating {len(articles)} articles to {target_lang}...")
    
    translated_count = 0
    failed_count = 0
    
    for i, article in enumerate(articles, 1):
        # è·³è¿‡å·²ç»æ˜¯ä¸­æ–‡çš„æ–‡ç« 
        if article.get('original_lang') == 'zh' or contains_chinese(article['title']):
            article['title_zh'] = article['title']
            article['summary_zh'] = article['summary']
            continue
        
        print(f"  [{i}/{len(articles)}] Translating: {article['title'][:50]}...")
        
        article['title_zh'] = translate_text(article['title'], 'en', target_lang)
        time.sleep(0.1)  # Ollama æœ¬åœ°è°ƒç”¨ï¼ŒçŸ­æš‚å»¶è¿Ÿå³å¯
        
        if article['summary']:
            article['summary_zh'] = translate_text(article['summary'], 'en', target_lang)
            time.sleep(0.1)
        else:
            article['summary_zh'] = ''
        
        # æ£€æŸ¥ç¿»è¯‘æ˜¯å¦æˆåŠŸ
        if article['title_zh'] != article['title']:
            translated_count += 1
        else:
            failed_count += 1
    
    print(f"âœ… Translated {translated_count} articles, {failed_count} unchanged")
    return articles

def deduplicate_articles(articles: list) -> list:
    """å»é‡ (åŸºäº ID)"""
    seen = set()
    unique = []
    
    for article in articles:
        if article['id'] not in seen:
            seen.add(article['id'])
            unique.append(article)
    
    return unique

def main():
    print("ğŸŒ Global News Fetcher started (with Ollama Translation)")
    print("=" * 50)
    
    # åŠ è½½é…ç½®
    sources_file = Path('src/sources.json')
    if not sources_file.exists():
        print(f"âŒ Config file not found: {sources_file}")
        return
    
    with open(sources_file, 'r', encoding='utf-8') as f:
        config = json.load(f)
    
    all_articles = []
    
    # æŠ“å–æ‰€æœ‰æº
    for i, source in enumerate(config['sources'], 1):
        print(f"[{i}/{len(config['sources'])}] Fetching {source['name']}...")
        articles = parse_rss_feed(source['rss'], source['name'], source.get('language', 'en'))
        
        # æ·»åŠ åˆ†ç±»ä¿¡æ¯
        for article in articles:
            article['categories'] = source.get('categories', ['general'])
            article['category'] = source.get('categories', ['general'])[0]
            article['country'] = source.get('country', 'US')
            article['language'] = source.get('language', 'en')
        
        all_articles.extend(articles)
        time.sleep(0.3)
    
    print("=" * 50)
    
    # å»é‡
    unique_articles = deduplicate_articles(all_articles)
    print(f"ğŸ“° Total articles: {len(all_articles)}")
    print(f"âœ¨ Unique articles: {len(unique_articles)}")
    
    # ç¿»è¯‘æ–‡ç«  (è‹±æ–‡â†’ä¸­æ–‡ï¼Œä½¿ç”¨æœ¬åœ° Ollama)
    unique_articles = translate_articles(unique_articles, 'zh')
    
    # æŒ‰æ—¶é—´æ’åº
    unique_articles.sort(
        key=lambda x: x.get('published', ''),
        reverse=True
    )
    
    # ç”Ÿæˆè¾“å‡º
    output = {
        'updated': datetime.now(timezone.utc).isoformat(),
        'total': len(unique_articles),
        'sources_count': len(config['sources']),
        'category_groups': config.get('categoryGroups', {}),
        'articles': unique_articles[:100]
    }
    
    # ä¿å­˜åˆ° data/news.json
    output_file = Path('data/news.json')
    output_file.parent.mkdir(exist_ok=True)
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(output, f, indent=2, ensure_ascii=False)
    
    print(f"âœ… Saved to {output_file}")
    print(f"ğŸ• Updated at: {output['updated']}")

if __name__ == '__main__':
    main()
