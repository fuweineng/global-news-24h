#!/usr/bin/env python3
"""
Global News Fetcher - å®¢è§‚ä¸€å¥è¯æ–°é—»
ä» RSS æºæŠ“å–æ–°é—»ï¼Œä½¿ç”¨ Qwen æ¨¡å‹ç”Ÿæˆå®¢è§‚çš„ä¸€å¥è¯æ‘˜è¦
"""

import feedparser
import json
from datetime import datetime, timezone
from pathlib import Path
import hashlib
import time
import re
import os
import urllib.request
from typing import List, Dict

DASHSCOPE_API_URL = "https://dashscope.aliyuncs.com/api/v1/services/aigc/text-generation/generation"

def get_api_key() -> str:
    """è·å– API Key"""
    if os.environ.get('DASHSCOPE_API_KEY'):
        return os.environ.get('DASHSCOPE_API_KEY')
    auth_file = Path.home() / '.openclaw' / 'agents' / 'main' / 'agent' / 'auth-profiles.json'
    if auth_file.exists():
        try:
            with open(auth_file, 'r') as f:
                auth = json.load(f)
                if 'dashscope' in auth and auth['dashscope'].get('apiKey'):
                    return auth['dashscope']['apiKey']
        except: pass
    return ""

def contains_chinese(text: str) -> bool:
    return bool(re.search(r'[\u4e00-\u9fff]', text))

def is_mostly_english(text: str) -> bool:
    if not text: return False
    letters = sum(1 for c in text if c.isalpha())
    if letters == 0: return False
    return sum(1 for c in text if c.isascii() and c.isalpha()) / letters > 0.8

def summarize_news_batch(articles: List[Dict], api_key: str) -> List[Dict]:
    """ä½¿ç”¨ Qwen ç”Ÿæˆå®¢è§‚ä¸€å¥è¯æ‘˜è¦"""
    if not api_key or not articles:
        for a in articles:
            a['one_line'] = a['title']
        return articles
    
    print(f"ğŸ¤– ç”Ÿæˆä¸€å¥è¯æ‘˜è¦ {len(articles)} ç¯‡...")
    
    # æ‰¹é‡å¤„ç†ï¼Œæ¯æ‰¹ 5 ç¯‡
    for i in range(0, len(articles), 5):
        batch = articles[i:i+5]
        input_text = "\n".join([f"{j+1}. {a['title']}" for j, a in enumerate(batch)])
        
        prompt = f"""ä½ æ˜¯å®¢è§‚æ–°é—»ç¼–è¾‘ã€‚å°†ä»¥ä¸‹æ–°é—»æ ‡é¢˜æ”¹å†™æˆå®¢è§‚çš„ä¸€å¥è¯æ–°é—»æ‘˜è¦ã€‚
è¦æ±‚ï¼š
- æ¯ç¯‡ä¸€è¡Œï¼Œä¿æŒåŸé¡ºåº
- å»æ‰ä¸»è§‚å½¢å®¹è¯ï¼ˆå¦‚"shocking", "amazing"ç­‰ï¼‰
- åªé™ˆè¿°äº‹å®ï¼Œä¸åŠ è¯„ä»·
- ä¸­æ–‡è¾“å‡ºï¼Œä¿ç•™è‹±æ–‡ä¸“æœ‰åè¯
- æ¯å¥ 20-40 å­—

æ–°é—»ï¼š
{input_text}

æ‘˜è¦ï¼š"""
        
        try:
            req = urllib.request.Request(
                DASHSCOPE_API_URL,
                data=json.dumps({
                    "model": "qwen-turbo",
                    "input": {"messages": [{"role": "user", "content": prompt}]},
                    "parameters": {"temperature": 0.1}
                }).encode(),
                headers={"Authorization": f"Bearer {api_key}", "Content-Type": "application/json"}
            )
            with urllib.request.urlopen(req, timeout=60) as resp:
                result = json.loads(resp.read().decode())
                lines = result['output']['choices'][0]['message']['content'].strip().split('\n')
                
                for j, article in enumerate(batch):
                    if j < len(lines):
                        line = re.sub(r'^\d+[\.\)]\s*', '', lines[j]).strip()
                        article['one_line'] = line if line else article['title']
                    else:
                        article['one_line'] = article['title']
        except Exception as e:
            print(f"âš ï¸ æ‘˜è¦å¤±è´¥ï¼š{e}")
            for a in batch:
                a['one_line'] = a['title']
        
        time.sleep(0.3)
    
    print("âœ… æ‘˜è¦å®Œæˆ")
    return articles

def fetch_news() -> List[Dict]:
    """æŠ“å–æ–°é—»"""
    sources_file = Path('src/sources.json')
    if not sources_file.exists():
        print("âŒ sources.json not found")
        return []
    
    with open(sources_file, 'r') as f:
        sources = json.load(f)['sources']
    
    articles = []
    print(f"ğŸ“° æŠ“å– {len(sources)} ä¸ªæº...")
    
    for source in sources:
        try:
            feed = feedparser.parse(source['rss'])
            for entry in feed.entries[:10]:
                # æ ¼å¼åŒ–æ—¶é—´ï¼šHH:MM
                pub_time = entry.get('published', '')
                try:
                    dt = datetime.fromisoformat(pub_time.replace('Z', '+00:00'))
                    time_str = dt.strftime('%H:%M')
                except:
                    time_str = '--:--'
                
                article = {
                    'id': hashlib.md5(f"{source['id']}-{entry.title}".encode()).hexdigest()[:12],
                    'title': entry.title,
                    'link': entry.link,
                    'published': entry.get('published', datetime.now(timezone.utc).isoformat()),
                    'time': time_str,  # HH:MM æ ¼å¼
                    'source': source['name'],
                    'summary': entry.get('summary', '')[:200],
                    'original_lang': source.get('language', 'en'),
                    'category': source['categories'][0] if source.get('categories') else 'world',
                    'country': source.get('country', 'US')
                }
                articles.append(article)
        except Exception as e:
            print(f"âš ï¸ {source['name']} å¤±è´¥ï¼š{e}")
        time.sleep(0.2)
    
    print(f"âœ… æŠ“å– {len(articles)} ç¯‡")
    return articles

def main():
    api_key = get_api_key()
    articles = fetch_news()
    if not articles:
        return
    
    # ç”Ÿæˆä¸€å¥è¯æ‘˜è¦
    articles = summarize_news_batch(articles, api_key)
    
    data = {
        'updated': datetime.now(timezone.utc).isoformat(),
        'total': len(articles),
        'sources_count': len(set(a['source'] for a in articles)),
        'articles': articles
    }
    
    data_dir = Path('data')
    data_dir.mkdir(exist_ok=True)
    
    with open(data_dir / 'news.json', 'w', encoding='utf-8') as f:
        json.dump(data, f, indent=2, ensure_ascii=False)
    
    print(f"âœ… ä¿å­˜åˆ° data/news.json")

if __name__ == '__main__':
    main()
