#!/usr/bin/env python3
"""
Global News Fetcher
ä» RSS æºæŠ“å–æ–°é—»ï¼Œç”Ÿæˆ JSON æ•°æ®
"""

import feedparser
import json
from datetime import datetime, timezone
from pathlib import Path
import hashlib
import time

def parse_rss_feed(url: str, source_name: str) -> list:
    """è§£æ RSS æº"""
    try:
        feed = feedparser.parse(url)
        articles = []
        
        for entry in feed.entries[:20]:  # æ¯ä¸ªæºå– 20 æ¡
            # è§£ææ—¶é—´
            published = ''
            if hasattr(entry, 'published_parsed') and entry.published_parsed:
                try:
                    dt = datetime(*entry.published_parsed[:6], tzinfo=timezone.utc)
                    published = dt.isoformat()
                except:
                    published = entry.get('published', '')
            
            article = {
                'id': hashlib.md5(entry.link.encode()).hexdigest()[:12],
                'title': entry.title,
                'link': entry.link,
                'published': published,
                'source': source_name,
                'summary': entry.get('summary', entry.get('description', ''))[:300]
            }
            articles.append(article)
        
        return articles
    except Exception as e:
        print(f"âŒ Error fetching {source_name}: {e}")
        return []

def deduplicate_articles(articles: list) -> list:
    """å»é‡ (åŸºäº ID)"""
    seen = set()
    unique = []
    
    for article in articles:
        if article['id'] not in seen:
            seen.add(article['id'])
            unique.append(article)
    
    return unique

def main():
    print("ğŸŒ Global News Fetcher started")
    print("=" * 50)
    
    # åŠ è½½é…ç½®
    sources_file = Path('src/sources.json')
    if not sources_file.exists():
        print(f"âŒ Config file not found: {sources_file}")
        return
    
    with open(sources_file, 'r', encoding='utf-8') as f:
        config = json.load(f)
    
    all_articles = []
    
    # æŠ“å–æ‰€æœ‰æº
    for i, source in enumerate(config['sources'], 1):
        print(f"[{i}/{len(config['sources'])}] Fetching {source['name']}...")
        articles = parse_rss_feed(source['rss'], source['name'])
        
        # æ·»åŠ åˆ†ç±»ä¿¡æ¯
        for article in articles:
            article['category'] = source['category']
            article['country'] = source['country']
            article['language'] = source['language']
        
        all_articles.extend(articles)
        time.sleep(0.5)  # é¿å…è¯·æ±‚è¿‡å¿«
    
    print("=" * 50)
    
    # å»é‡
    unique_articles = deduplicate_articles(all_articles)
    print(f"ğŸ“° Total articles: {len(all_articles)}")
    print(f"âœ¨ Unique articles: {len(unique_articles)}")
    
    # æŒ‰æ—¶é—´æ’åº
    unique_articles.sort(
        key=lambda x: x.get('published', ''),
        reverse=True
    )
    
    # ç”Ÿæˆè¾“å‡º
    output = {
        'updated': datetime.now(timezone.utc).isoformat(),
        'total': len(unique_articles),
        'sources_count': len(config['sources']),
        'articles': unique_articles[:100]  # åªä¿ç•™æœ€æ–° 100 æ¡
    }
    
    # ä¿å­˜åˆ° data/news.json
    output_file = Path('data/news.json')
    output_file.parent.mkdir(exist_ok=True)
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(output, f, indent=2, ensure_ascii=False)
    
    print(f"âœ… Saved to {output_file}")
    print(f"ğŸ• Updated at: {output['updated']}")

if __name__ == '__main__':
    main()
