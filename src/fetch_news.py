#!/usr/bin/env python3
"""å…¨çƒæ–°é—»æŠ“å– - MyMemory å…è´¹ç¿»è¯‘ + æ™ºèƒ½å»é‡"""

import feedparser
import json
from datetime import datetime, timezone, timedelta
from pathlib import Path
import hashlib
import time
import urllib.request
import urllib.parse
import re

MYMEMORY_API = "https://api.mymemory.translated.net/get"
CACHE_FILE = Path('data/news_cache.json')

def normalize_title(title):
    """æ ‡å‡†åŒ–æ ‡é¢˜ç”¨äºå»é‡æ¯”è¾ƒ"""
    # è½¬å°å†™ï¼Œç§»é™¤ç‰¹æ®Šå­—ç¬¦ï¼Œä¿ç•™å­—æ¯æ•°å­—
    title = title.lower()
    title = re.sub(r'[^a-z0-9\s]', '', title)
    # ç§»é™¤å¤šä½™ç©ºæ ¼
    title = ' '.join(title.split())
    return title

def load_cache():
    """åŠ è½½ç¼“å­˜çš„æ–°é—» ID"""
    if CACHE_FILE.exists():
        with open(CACHE_FILE, 'r') as f:
            return json.load(f)
    return {'articles': [], 'updated': None}

def save_cache(data):
    """ä¿å­˜ç¼“å­˜"""
    CACHE_FILE.parent.mkdir(exist_ok=True)
    with open(CACHE_FILE, 'w') as f:
        json.dump(data, f, indent=2)

def translate_batch(articles):
    """ä½¿ç”¨ MyMemory ç¿»è¯‘æ–°é—»æ ‡é¢˜ä¸ºä¸­æ–‡"""
    if not articles:
        return articles
    
    print(f"ğŸ¤– MyMemory ç¿»è¯‘ {len(articles)} ç¯‡...")
    
    for i, article in enumerate(articles):
        try:
            url = f"{MYMEMORY_API}?q={urllib.parse.quote(article['title'])}&langpair=en|zh"
            req = urllib.request.Request(url)
            with urllib.request.urlopen(req, timeout=30) as resp:
                result = json.loads(resp.read().decode())
                translation = result.get('responseData', {}).get('translatedText', '')
                article['one_line'] = translation if translation else article['title']
        except Exception as e:
            print(f"âš ï¸ ç¿»è¯‘å¤±è´¥ï¼š{e}")
            article['one_line'] = article['title']
        time.sleep(0.15)
        
        if (i + 1) % 20 == 0:
            print(f"  å·²ç¿»è¯‘ {i+1}/{len(articles)} ç¯‡")
    
    print("âœ… ç¿»è¯‘å®Œæˆ")
    return articles

def fetch_news():
    sources_file = Path('src/sources.json')
    if not sources_file.exists():
        print("âŒ sources.json not found")
        return []
    
    with open(sources_file, 'r') as f:
        sources = json.load(f)['sources']
    
    # æŒ‰ä¼˜å…ˆçº§æ’åºï¼ˆæ•°å­—è¶Šå°ä¼˜å…ˆçº§è¶Šé«˜ï¼‰
    sources.sort(key=lambda s: s.get('priority', 99))
    
    # åŠ è½½ç¼“å­˜ï¼Œç”¨äºå»é‡
    cache = load_cache()
    cached_ids = {a['id'] for a in cache.get('articles', [])}
    
    articles = []
    seen_normalized = set()  # æ ‡å‡†åŒ–åçš„æ ‡é¢˜ç”¨äºå»é‡
    print(f"ğŸ“° æŠ“å– {len(sources)} ä¸ªæº...")
    
    for source in sources:
        try:
            feed = feedparser.parse(source['rss'])
            for entry in feed.entries[:10]:  # æ¯ä¸ªæºæŠ“å– 10 ç¯‡
                # ç”Ÿæˆå”¯ä¸€ ID
                article_id = hashlib.md5(f"{source['id']}-{entry.title}".encode()).hexdigest()[:12]
                
                # è·³è¿‡ç¼“å­˜ä¸­å·²æœ‰çš„æ–‡ç« 
                if article_id in cached_ids:
                    continue
                
                # æ ‡å‡†åŒ–æ ‡é¢˜å»é‡ï¼ˆå¤„ç†ä¸åŒæ¥æºçš„ç›¸åŒæ–°é—»ï¼‰
                normalized = normalize_title(entry.title)
                if normalized in seen_normalized:
                    print(f"  âš ï¸ è·³è¿‡é‡å¤ï¼š{entry.title[:40]}...")
                    continue
                seen_normalized.add(normalized)
                
                try:
                    dt = datetime.fromisoformat(entry.get('published', '').replace('Z', '+00:00'))
                    time_str = dt.strftime('%H:%M')
                except:
                    time_str = '--:--'
                
                article = {
                    'id': article_id,
                    'title': entry.title,
                    'link': entry.link,
                    'published': entry.get('published', datetime.now(timezone.utc).isoformat()),
                    'time': time_str,
                    'source': source['name'],
                    'summary': entry.get('summary', '')[:200],
                    'category': source['categories'][0] if source.get('categories') else 'world',
                    'original_lang': source.get('language', 'en'),
                    'priority': source.get('priority', 2)
                }
                articles.append(article)
        except Exception as e:
            print(f"âš ï¸ {source['name']} å¤±è´¥ï¼š{e}")
        time.sleep(0.2)
    
    # æŒ‰ä¼˜å…ˆçº§å’Œæ—¶é—´æ’åº
    articles.sort(key=lambda a: (a.get('priority', 2), a['published']), reverse=False)
    
    # æ›´æ–°ç¼“å­˜ï¼ˆä¿ç•™æœ€è¿‘ 500 ç¯‡ï¼‰
    cache['articles'] = articles[:500]
    cache['updated'] = datetime.now(timezone.utc).isoformat()
    save_cache(cache)
    
    print(f"âœ… æŠ“å– {len(articles)} ç¯‡ï¼ˆå»é‡åï¼Œç¼“å­˜ {len(cache['articles'])} ç¯‡ï¼‰")
    return articles

def main():
    articles = fetch_news()
    if not articles:
        print("â„¹ï¸ æ— æ–°æ–°é—»ï¼Œè·³è¿‡æ›´æ–°")
        return
    
    articles = translate_batch(articles)
    
    data = {
        'updated': datetime.now(timezone.utc).isoformat(),
        'total': len(articles),
        'articles': articles
    }
    
    data_dir = Path('data')
    data_dir.mkdir(exist_ok=True)
    with open(data_dir / 'news.json', 'w', encoding='utf-8') as f:
        json.dump(data, f, indent=2, ensure_ascii=False)
    print(f"âœ… ä¿å­˜åˆ° data/news.json")

if __name__ == '__main__':
    main()
